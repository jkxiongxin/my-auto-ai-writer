#!/usr/bin/env python3
"""è®¾ç½®å’Œæµ‹è¯•è‡ªå®šä¹‰å¤§æ¨¡å‹æ¥å£çš„è„šæœ¬."""

import os
import sys
import asyncio
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def create_custom_env_file():
    """åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹çš„ç¯å¢ƒé…ç½®æ–‡ä»¶."""
    
    print("ğŸ”§ é…ç½®è‡ªå®šä¹‰å¤§æ¨¡å‹æ¥å£")
    print("="*60)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    print("\nè¯·è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰æ¨¡å‹é…ç½®ä¿¡æ¯:")
    
    base_url = input("ğŸ“¡ æ¨¡å‹APIåœ°å€ (å¦‚: http://localhost:8080/v1): ").strip()
    if not base_url:
        print("âŒ APIåœ°å€ä¸èƒ½ä¸ºç©º")
        return False
    
    api_key = input("ğŸ”‘ APIå¯†é’¥ (å¯é€‰ï¼Œå¦‚æ— éœ€è®¤è¯è¯·ç›´æ¥å›è½¦): ").strip()
    model_name = input("ğŸ¤– æ¨¡å‹åç§° (å¦‚: llama2-7b-chat): ").strip()
    if not model_name:
        model_name = "custom-model"
    
    api_format = input("ğŸ“‹ APIæ ¼å¼ (openai/customï¼Œé»˜è®¤openai): ").strip().lower()
    if api_format not in ["openai", "custom"]:
        api_format = "openai"
    
    timeout = input("â±ï¸ è¯·æ±‚è¶…æ—¶æ—¶é—´/ç§’ (é»˜è®¤300): ").strip()
    try:
        timeout = int(timeout) if timeout else 300
    except ValueError:
        timeout = 300
    
    # åˆ›å»º.envæ–‡ä»¶å†…å®¹
    env_content = f"""# AIæ™ºèƒ½å°è¯´ç”Ÿæˆå™¨é…ç½®
# è‡ªå®šä¹‰å¤§æ¨¡å‹é…ç½®

# =============================================================================
# åŸºæœ¬é…ç½®
# =============================================================================
ENVIRONMENT=development
DEBUG=true
LOG_LEVEL=INFO
LOG_FORMAT=json

# =============================================================================
# APIæœåŠ¡é…ç½®
# =============================================================================
HOST=0.0.0.0
PORT=8000
API_RELOAD=true

# =============================================================================
# æ•°æ®åº“é…ç½®
# =============================================================================
DATABASE_URL=sqlite+aiosqlite:///./ai_novel_generator.db

# =============================================================================
# LLMæä¾›å•†é…ç½® â­ é‡ç‚¹é…ç½®
# =============================================================================
PRIMARY_LLM_PROVIDER=custom
FALLBACK_LLM_PROVIDERS=ollama

# =============================================================================
# è‡ªå®šä¹‰æ¨¡å‹é…ç½® â­ æ‚¨çš„é…ç½®
# =============================================================================
CUSTOM_MODEL_BASE_URL={base_url}
CUSTOM_MODEL_API_KEY={api_key if api_key else "none"}
CUSTOM_MODEL_NAME={model_name}
CUSTOM_MODEL_TIMEOUT={timeout}
CUSTOM_MODEL_API_FORMAT={api_format}
CUSTOM_MODEL_AUTH_TYPE=bearer
CUSTOM_MODEL_MAX_TOKENS=4000
CUSTOM_MODEL_TEMPERATURE=0.7

# =============================================================================
# Ollamaé…ç½® (åå¤‡é€‰é¡¹)
# =============================================================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2:7b-chat
OLLAMA_TIMEOUT=300

# =============================================================================
# æ€§èƒ½é…ç½®
# =============================================================================
MAX_CONCURRENT_GENERATIONS=2
GENERATION_TIMEOUT=3600
CHAPTER_GENERATION_TIMEOUT=600

# =============================================================================
# CORSé…ç½®
# =============================================================================
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8000
CORS_ALLOW_CREDENTIALS=true
CORS_ALLOW_METHODS=GET,POST,PUT,DELETE,OPTIONS
CORS_ALLOW_HEADERS=*

# =============================================================================
# ç¼“å­˜é…ç½®
# =============================================================================
CACHE_ENABLED=true
REQUEST_CACHE_TTL=1800

# =============================================================================
# æ—¥å¿—é…ç½®
# =============================================================================
LOG_FILE=logs/ai_novel_generator.log
"""
    
    # å†™å…¥.envæ–‡ä»¶
    try:
        with open('.env', 'w', encoding='utf-8') as f:
            f.write(env_content)
        print(f"\nâœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜åˆ° .env")
        print(f"ğŸ“¡ APIåœ°å€: {base_url}")
        print(f"ğŸ¤– æ¨¡å‹åç§°: {model_name}")
        print(f"ğŸ“‹ APIæ ¼å¼: {api_format}")
        print(f"â±ï¸ è¶…æ—¶æ—¶é—´: {timeout}ç§’")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return False

async def test_custom_model():
    """æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹è¿æ¥."""
    
    print("\nğŸ§ª æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹è¿æ¥...")
    print("-" * 40)
    
    try:
        # å¯¼å…¥ç›¸å…³æ¨¡å—
        from src.utils.llm_client import UniversalLLMClient
        from src.utils.providers.router import TaskType
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = UniversalLLMClient()
        
        # æµ‹è¯•æä¾›å•†å¥åº·çŠ¶æ€
        print("1. æ£€æŸ¥æä¾›å•†çŠ¶æ€...")
        provider_stats = await client.test_providers()
        
        for provider_name, status in provider_stats.items():
            health_icon = "âœ…" if status.get('healthy', False) else "âŒ"
            print(f"   {health_icon} {provider_name}: {status.get('error', 'æ­£å¸¸')}")
        
        # æµ‹è¯•ç®€å•ç”Ÿæˆ
        print("\n2. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        test_prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½çš„æ¦‚å¿µã€‚"
        
        try:
            result = await client.generate(
                prompt=test_prompt,
                task_type=TaskType.GENERAL,
                preferred_provider="custom",
                max_tokens=100,
                temperature=0.7
            )
            
            print(f"âœ… ç”ŸæˆæˆåŠŸ!")
            print(f"ğŸ“ æç¤ºè¯: {test_prompt}")
            print(f"ğŸ¤– å›å¤: {result[:200]}{'...' if len(result) > 200 else ''}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_novel_generation():
    """æµ‹è¯•å°è¯´ç”ŸæˆåŠŸèƒ½."""
    
    print("\nğŸ“š æµ‹è¯•å°è¯´ç”ŸæˆåŠŸèƒ½...")
    print("-" * 40)
    
    try:
        from src.core.novel_generator import NovelGenerator
        from src.utils.llm_client import UniversalLLMClient
        
        # åˆ›å»ºç”Ÿæˆå™¨
        llm_client = UniversalLLMClient()
        generator = NovelGenerator(llm_client)
        
        # æµ‹è¯•æ¦‚å¿µæ‰©å±•
        print("1. æµ‹è¯•æ¦‚å¿µæ‰©å±•...")
        test_input = "ä¸€ä¸ªå…³äºæ—¶é—´æ—…è¡Œçš„ç§‘å¹»æ•…äº‹"
        
        try:
            concept = await generator.concept_expander.expand_concept(
                user_input=test_input,
                target_words=1000,
                style_preference="ç§‘å¹»"
            )
            
            print(f"âœ… æ¦‚å¿µæ‰©å±•æˆåŠŸ!")
            print(f"ğŸ“– ä¸»é¢˜: {concept.theme}")
            print(f"ğŸ¯ æ ¸å¿ƒå†²çª: {concept.core_conflict}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¦‚å¿µæ‰©å±•å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ å°è¯´ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def show_usage_guide():
    """æ˜¾ç¤ºä½¿ç”¨æŒ‡å—."""
    
    print("\nğŸ“– è‡ªå®šä¹‰æ¨¡å‹ä½¿ç”¨æŒ‡å—")
    print("="*60)
    
    print("""
ğŸ¯ æ”¯æŒçš„APIæ ¼å¼:

1. OpenAIå…¼å®¹æ ¼å¼ (æ¨è):
   - é€‚ç”¨äºå¤§å¤šæ•°å¼€æºæ¨¡å‹ (å¦‚ vLLM, FastChat, Ollamaç­‰)
   - è¯·æ±‚æ ¼å¼: POST /v1/chat/completions
   - å“åº”æ ¼å¼: OpenAIæ ‡å‡†æ ¼å¼

2. è‡ªå®šä¹‰æ ¼å¼:
   - é€‚ç”¨äºç‰¹æ®ŠAPIæ¥å£
   - å¯é…ç½®è¯·æ±‚å’Œå“åº”æ ¼å¼

ğŸ”§ å¸¸è§æ¨¡å‹éƒ¨ç½²å·¥å…·é…ç½®:

1. vLLM:
   python -m vllm.entrypoints.openai.api_server \\
     --model your-model-path \\
     --host 0.0.0.0 \\
     --port 8080

2. FastChat:
   python -m fastchat.serve.openai_api_server \\
     --host 0.0.0.0 \\
     --port 8080

3. Ollama:
   ollama serve
   # é»˜è®¤ç«¯å£: 11434

ğŸš€ å¯åŠ¨æ­¥éª¤:

1. é…ç½®ç¯å¢ƒå˜é‡: python setup_custom_model.py
2. æµ‹è¯•è¿æ¥: python test_logging.py  
3. å¯åŠ¨æœåŠ¡: python start_api.py
4. è®¿é—®ç•Œé¢: http://localhost:8000/app

ğŸ’¡ æ•…éšœæ’é™¤:

- è¿æ¥è¶…æ—¶: æ£€æŸ¥ç½‘ç»œå’Œé˜²ç«å¢™è®¾ç½®
- è®¤è¯å¤±è´¥: ç¡®è®¤APIå¯†é’¥æ­£ç¡®
- æ ¼å¼é”™è¯¯: æ£€æŸ¥APIæ ¼å¼é…ç½®
- ç”Ÿæˆå¤±è´¥: æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶æ’æŸ¥å…·ä½“é”™è¯¯

ğŸ“ ç›¸å…³æ–‡ä»¶:
- .env: ç¯å¢ƒé…ç½®
- logs/ai_novel_generator.log: è¿è¡Œæ—¥å¿—
- .env.custom_model_example: é…ç½®ç¤ºä¾‹
""")

def main():
    """ä¸»å‡½æ•°."""
    
    print("ğŸ¤– AIæ™ºèƒ½å°è¯´ç”Ÿæˆå™¨ - è‡ªå®šä¹‰æ¨¡å‹é…ç½®å·¥å…·")
    print("="*60)
    
    if len(sys.argv) > 1 and sys.argv[1] == "--guide":
        show_usage_guide()
        return
    
    # 1. é…ç½®ç¯å¢ƒæ–‡ä»¶
    if not create_custom_env_file():
        return
    
    # 2. æµ‹è¯•è¿æ¥
    print(f"\nğŸ” å¼€å§‹æµ‹è¯•é…ç½®...")
    
    try:
        # æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹
        success = asyncio.run(test_custom_model())
        
        if success:
            print(f"\nğŸ‰ è‡ªå®šä¹‰æ¨¡å‹é…ç½®æˆåŠŸ!")
            
            # æµ‹è¯•æ¦‚å¿µæ‰©å±•
            concept_success = asyncio.run(test_novel_generation())
            
            if concept_success:
                print(f"\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! æ‚¨å¯ä»¥å¼€å§‹ä½¿ç”¨AIå°è¯´ç”Ÿæˆå™¨äº†ã€‚")
                print(f"\nğŸš€ å¯åŠ¨å‘½ä»¤: python start_api.py")
                print(f"ğŸŒ è®¿é—®åœ°å€: http://localhost:8000/app")
            else:
                print(f"\nâš ï¸ åŸºç¡€è¿æ¥æ­£å¸¸ï¼Œä½†å°è¯´ç”ŸæˆåŠŸèƒ½å¯èƒ½éœ€è¦è°ƒè¯•")
        else:
            print(f"\nâŒ é…ç½®æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ‚¨çš„APIè®¾ç½®")
            
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ é…ç½®å·²ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é…ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main()